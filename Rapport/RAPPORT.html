<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=768, initial-scale=1">
        <script src="http://distill.pub/template.v1.js"></script>
        <script type="text/front-matter">
          title: Deep Image Analogy
          description: A Semantically Consistent Approach to Neural Style Transfer
          authors:
            - Julien Roy: https://github.com/julienroy13
            - David Kanaa: https://github.com/davidkanaa
          affiliations:
              - Polytechnique Montreal: "http://www.polymtl.ca"
              - Polytechnique Montreal: "http://www.polymtl.ca"
        </script>
        <!-- Katex -->
        <script src="assets/lib/auto-render.min.js"></script>
        <script src="assets/lib/katex.min.js"></script>
        <link rel="stylesheet" href="assets/lib/katex.min.css">
        <link rel="stylesheet" type="text/css" href="assets/widgets.css">
        <link rel="stylesheet" type="text/css" href="assets/main.css">

        <!-- custom -->
        <link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">

        <!-- Required -->
        <script src="assets/lib/lib.js"></script>
        <script src="assets/utils.js"></script>
        <script>
        var renderQueue = [];

        function renderMath(elem) {
            renderMathInElement(
                elem, {
                    delimiters: [{
                        left: "$$",
                        right: "$$",
                        display: true
                    }, {
                        left: "$",
                        right: "$",
                        display: false
                    }, ]
                }
            );
        }

        var deleteQueue = [];

        function renderLoading(figure) {
            var loadingScreen = figure.append("svg")
                .style("width", figure.style("width"))
                .style("height", figure.style("height"))
                .style("position", "absolute")
                .style("top", "0px")
                .style("left", "0px")
                .style("background", "white")
                .style("border", "0px dashed #DDD")
                .style("opacity", 1)

            return function(callback) {
                loadingScreen
                    .remove()
            }

        }
        </script>

        <style>

        #previews figcaption {
          text-align: center;
        }

        .tex, .tex > * {
          font-family: "Computer Modern Serif", serif;
        }

        p, p > * {
          text-align: justify;
        }

        .section-title, .sub-section-title {
          font-weight: bold;
        }

        </style>

    </head>
    <body>
        <dt-article class="left tex">
            <h1>Deep Image Analogy</h1>
            <h2>A Semantically Consistent Approach to Neural Style Transfer</h2>
            <dt-byline></dt-byline>

                <p>
                    Image style transfer refers to the problem of transforming an image in such a way that it acquires the stylistic attributes of a reference image, while preserving its content information. For instance, color editing with respect to a reference image can be achieved by matching the target and the reference color histograms <dt-cite key="pouli2010"></dt-cite>. However, even if defining a complete definition of <i>style</i> would be difficult, most would agree that the stylistic attributes account for more than simply color. Some of these attributes however might be hard to describe or identify -- the style of a painting could lie in the brush strokes and the textures, the style of a photograph is partly defined by the lighting and tone nuances.
                </p>
                <p>
                    The fact that humans can appreciate and differentiate style without quite agreeing on how to describe it (even less on how to formalize it) makes machine learning a promising approach for that problem.
                </p>
                <p>
                    Gatys <i>et al.</i> developped the first algorithm that uses a deep convolutional neural network (CNN) to capture and modelize the style of an image. This technique is refered to as <dt-cite key="gatys2016">Neural Style Transfer</dt-cite>. However, while being able to produce impressive results, this approach fails to carry out style transfer in a manner that preserves the semantic correspondances between the two original images. For example, the algorithm won't distinguish the difference between the background and the main objects of both scenes, and thus will apply the style in an homogenous manner to the generated image without any regard about where which part of the style should be applied.
                </p>
                <p>
                    Liao <i>et al.</i> solve that issue by introducing <dt-cite key="liao2017">Deep Image Analogy</dt-cite>. This algorithm is inspired by the original Neural Style Transfer as it still uses a pretrained deep CNN to extract information about the images, but also cleverly apply the Patch Match method<dt-cite key="barnes2009"></dt-cite> on deep feature maps constructed by that CNN to take into account semantic correspondances between the two base images. Therefore, they iteratively progress in a coarse-to-fine manner (from the deeper to shallower representations) to generate the new latent image that both presents the style of one reference, and the content of another, in a semantically consistent fashion.
                </p>
                <hr> <!--Separation line-->

            <!-- SECTION ____________________________________________________________________________-->
            <h2 class="section-title">Background</h2>
                <p> In this section, we briefly present the pre-requisite notions that are at the core of the Deep Image Analogy method.  </p>

            <h3 class="sub-section-title">1. Randomized Patch-Match algorithm</h3>
                <p class="toRender">
                    Patch matching refers to the problem of finding the nearest neighbor patch in an image <i>B</i> to a source patch in an image <i>A</i>. The result of this process is a mapping function $\phi_{a \to b}$ called Nearest-Neighbor Field (NNF) that is of the same size as image <i>A</i>, and which defines a mapping from pixels in <i>A</i> to pixels in <i>B</i> : let <i><b>p</b></i> be a pixel in <i>A</i> and $N(p)$ a fixed patch centered on $p$, $\phi_{a \to b}(p)=q$ is the pixel in <i>B</i> such that $N(q)$ is the nearest patch in <i>B</i> to $N(p)$, with respect to some distance function $d(.,.)$. The patches are usually extended to all channels of an image, so for a 3 channels RGB image, a $3\times3$ patch would in fact refer to a $3\times3\times3$ tensor. Different similarity metrics can be used to define the nearest patch may be used, a simple one being the squared Euclidean distance :

                    $$ d(P^{A}, P^{B}) = \| P^{A}-P^{B} \|_{2}^{2}= \sum_{i,j,k} (P^{A}_{ijk} - P^{B}_{ijk})^{2}$$
                </p>
                <p>
                    The main challenge lies in the intractable number of patches that needs to be compared with each other, which grows exponentially with the size of the images. Luckily, the Randomized approach to <dt-cite key="barnes2009">PatchMatch</dt-cite> allows to find very good estimate solutions in a much shorter amount of time. The strategy is simple. First, the NNF is initialized randomly. Then, for each pixels, we execute two steps : propagation and random search. Propagation refers to looking at the pixel's neighbors to see if they could find a better match than its own. Random search consists in randomly selecting potential match candidates in a certain area, and see if they offer a better match than the current one. Those steps are illustrated in the next figure.

                    <figure>
                        <center><img class="image" src="images/patch_match.png" height="400px"></center>
                        <figcaption style="text-align: center;">Randomized Patch-Match illustrated as a 3-step process (Figure from :<dt-cite key="barnes2009"></dt-cite>)</figcaption>
                    </figure>
                </p>
                <p>
                    This approach depends on the reasonable assumption that natural images are usually spatially smooth. That means that small sub-regions of the image will usually have similar colors, so successive patches in image <i>A</i> might want to share the same correspondances in image <i>B</i> (propagation step) and that comparing with randomly sampled patches from different sub-regions counts as an effective scan of the image in order to find a good neighborhood (random search step). When the PatchMatch is successful, one can express image <i>A</i> only in terms of pixels in image <i>B</i>, a process shown in the next figure. This resulting hybrid image is called a <i>warp</i>.
                </p>
                <figure>
                    <center><img class="image" src="images/warp.png"></center>
                    <figcaption class="toRender" style="text-align: center;">
                        The result of applying the Randomized PatchMatch algorithm on two images : it maps <i>image A</i> to <i>image B</i>.
                        A warping operation using the constructed NNF is applied on <i>image B</i> to recover <i>image A</i>.
                    </figcaption>
                </figure>

            <h3 class="sub-section-title">2. Deep image representations in CNNs</h3>
                <!-- here give some background info on deep CNNs used in image recognition tasks and VGG trained for classification and recently used for style Transfer -->
                <p class="toRender">
                  <dt-cite key="SimonyanZ14a">VGG-19</dt-cite> is a deep convolutional neural network architecture which have been trained and evaluated on the ImageNet data set for image classification/recognition tasks, and demonstrated <i>state-of-the-art</i> performance on such tasks in 2015. In short, the network consists of 5 convolutional blocks (made of 2, 2, 4, 4 and 4 layers respectively) each one followed by a $2\times2$ pooling layer, and ends with a few fully-connected layers. Each block applies further transformations on the output of the previous one, extracting higher and higher level information that expose latent semantic information (such as eyes and nose positions for faces, textural representations, etc). This high level information is later used by the fully connected layers for classification.
                </p>
                <figure id="" style="padding:5px 5px 0px 0px">
                  <center style="position:relative; top:-35px">
                    <img class="image" src="images/vgg.png" hight="400px">
                  </center>
                  <figcaption class="toRender" style="text-align: center;">
                      An illustration of the VGG deep neural architecture.
                  </figcaption>

                </figure>
                <p class="toRender">
                  We use a pretrained instance of the VGG-19 architecture to build and extract feature maps. Since the network is not used for classification purposes, we can get rid of the fully connected layers, and only work with the convolutional blocks of VGG.
                  We do not retrieve the feature maps produced at the output of every layer of the network, but only those produced at the first layer of convolutional blocks (the output of the $reluL\_1$).
                </p>
                <figure id="" style="padding:5px 5px 0px 0px">
                  <center style="position:relative; top:-35px">
                    <img class="image" src="images/deep_features.png" hight="400px">
                  </center>
                  <figcaption class="toRender" style="text-align: center;">
                      An illustration of feature maps $F^L, \forall L \in \lbrace 1, 2, ..., 5\rbrace$ extracted with the <dt-cite key="SimonyanZ14a">VGG-19</dt-cite> architecture.
                  </figcaption>
                </figure>

            <hr> <!--Separation line-->
            <!-- SECTION ____________________________________________________________________________-->
            <h2 class="section-title">Algorithm</h2>
                <p class="toRender">
                    The algorithm has been designed to perform style transfer on two images, while respecting the semantic correspondances between them. Given two reference images <i>A1</i> and <i>B1</i>, it produces two mapping functions $\phi_{a \to b}$ and $\phi_{b \to a}$ that allow to warp the references into two new images <i>A2</i> and <i>B2</i> that mix the style and the content of the references. An overview of the algorithm is presented on the following figure.
                </p>
                <figure>
                    <center><img class="image" src="images/overview.png"></center>
                    <figcaption style="text-align: center;">Overview of Deep Image Analogy (Figure from :<dt-cite key="liao2017"></dt-cite>)</figcaption>
                </figure>
                <p class="toRender">
                    Broadly, the process aims at reconstructing the deep feature maps of both latent images, starting at the fifth layer and working its the way back to the first layer of VGG. Therefore, before starting this process, the two reference images are fed to the network and their deep features are saved. The feature maps of the two latent images at the deepest layer ($L=5$) are initialized to the same values as the reference images (more specifically, $F^{(5)}_{A2}$ is initialized with $F^{(5)}_{A1}$ and $F^{(5)}_{B2}$ is initialized with $F^{(5)}_{B1}$). Then, for each layer $L$, an NNF-search (Deep PatchMatch) is performed to discover the mapping functions $\phi^{(L)}_{a \to b}$ and $\phi^{(L)}_{b \to a}$, followed by an optimization phase (Deep Reconstruction) that aims at reconstructing the feature maps $F^{(L-1)}_{A2}$ and $F^{(L-1)}_{B2}$ of the images at layer $L-1$. The Deep PatchMatch and Deep Reconstruction phases are explained in more details in the next sections.
                </p>

            <h3 class="sub-section-title">1. Deep PatchMatch</h3>
                <p>
                    PatchMatch approach usually operates on raw pixels. That allows to find color correspondances between images but does not take into account the inherent semantic structure of the provided images.
                </p>

                <p>
                    One key element of the DeepImageAnalogy method is that the PatchMatch is applied to find correspondances between the deep features of the reference and latent images. Applying this technique at such a deep level of image features allows to map semantic elements of both images to one another. The next figure shows how the eyes of the two reference images are very similarly represented at that depth.
                </p>
                <figure>
                    <center><img class="image" src="images/semantic_correspondances.png"></center>
                    <figcaption style="text-align: center;">Both faces, very dissimilar at the pixel level, are now represented in very similar ways at the 5th layer. (Images from <dt-cite key="liao2017"></dt-cite>)</figcaption>
                </figure>
                <p class="toRender">
                    Another key element is that the random search in deep patch match is locally constrained. The mapping function for the fifth layer is randomly initialized and has access to the whole image in the random search phase in order to find the best possible matches for each semantic attributes. However, the mapping functions for $L \in \{1,2,3,4\}$ are initilized with an upsampled version of the previous layer, and their random search only has access to a small region centered at the current best match, making it impossible to completely destroy the mapping that had been established in previous layers. Therefore, the correspondances found at the deepest layers serve as the foundation upon which the next mapping functions are built, and those shallower layers mapping only are fine-tuned versions of the deeper ones. This fine-tuning effect is illustrated in the following figure.
                </p>
              <figure>
                    <center><img class="image" src="images/spatial_constraint.png"></center>
                    <figcaption style="text-align: center;">The fifth layer finds the semantic correspondances. This mapping is then fine-tuned to fit smaller details and textures as the process takes us to shallower and shallower layers.</figcaption>
              </figure>

            <h3 class="sub-section-title">2. Deep Reconstruction</h3>
                <!-- here explain every step an notion involved in the feature reconstruction -->
                <p class="toRender">
                  For every layer $L \in \lbrack 2..5 \rbrack$, we use the pair of feature maps $F_{A1}^{L-1}$ and $F_{B1}^{L}$, respectively of image <i>A1</i> at layer $L-1$ and <i>B1</i> at layer $L$.
                  The goal of the feature reconstruction is to recover the features of <i>A2</i> at layer $L-1$. This phase consists in 3 steps : <i>(1)</i> the feature map warping using NNFs computed in current layer, <i>(2)</i> the deconvolution and <i>(3)</i> the feature blending.
                </p>
                <figure id="" style="padding:5px 5px 0px 0px">
                  <center style="position:relative; top:-35px">
                    <img class="image" src="images/reconstruction.png" hight="400px">
                  </center>
                  <figcaption class="toRender" style="text-align: center;">
                      An overview of the feature reconstruction process.
                  </figcaption>
                </figure>
                <h4>Feature warping</h4>
                <p class="toRender">
                  The warp of image <i>B1</i> using $\phi^{L}_{a \to b}$ should give feature maps that have the approximate content than the feature maps of <i>A1</i>, but exprimed only in terms of patches takent from the feature maps of <i>B1</i>. Provided that we have this $\phi^{L}_{a \to b}$, the nearest neighbour field at layer $L$, we apply a warping operation to $F_{B1}^{L}$ such that:
                  $$\forall \text{ pixel } p \in F_{B1}^{L}, \text{ warp}(F_{B1}^{L}(p)) = F_{B1}^{L}(\phi_{a \to b}(p))$$
                  Therefore, the warped feature maps represents a version of $F_{B1}^{L}$ where every semantic concept has been shifted to match the location of its correspondant in $F_{A1}^{L}$.
                  It makes sense that if we are to later blend feature maps of both images we must make sure that their matching semantic concepts have the same location, in order for the resultant feature map to keep some consistency in terms of semantic information; in other words they must have the same structure.
                </p>
                <figure id="" style="padding:5px 5px 0px 0px">
                  <center style="position:relative; top:-35px">
                    <img class="image" src="images/warping.png" hight="400px">
                  </center>
                  <figcaption class="toRender" style="text-align: center;">
                      An illustration of the effect of warping (here using the NNFs of layer 5).
                  </figcaption>
                </figure>

                <h4>Deconvolution</h4>
                <p class="toRender">
                  Since the goal is to recover $F_{A2}^{L-1}$, that is the feature maps at the previous layer, we must deconvolve the features of the current layer <i>L</i> using the appropriate subnet $CNN_{L-1}^{L}$ of VGG-19. This deconvolution task is here formulated as an optimization problem, in which the targets are the warped features $F_{B1}^{L}(\phi_{a \to b})$, and the loss function :
                  $$ \mathcal{L}_{{R_{B1}^{L-1}}} = \| CNN_{L-1}^{L}(R_{B1}^{L}) - F_{B1}^{L}(\phi_{a \to b}^{L}) \|^{2} $$
                  Using the convolutional network here allows, again, to take advantage of the local consistency of the semantic information in features. As the network already knows how to build the feature maps and extract semantic information, it can be used in this fashion, to reverse its doing. Via the deconvolution viewed as an optimization problem, we can determine the features $R_{B1}^{L}$ which should be fed into the previous layer in order to obtain the warped features we have at the current layer. We initialize $R_{B1}^{L}$ with a random [gaussian] noise, and solve the numerical optimization problem through gradient descent $\partial \mathcal{L}_{{R_{B1}^{L-1}}}/{ \partial R_{B1}^{L-1} }$.
                </p>
                <p class="toRender">
                  Though the <dt-cite key="Zhu1997">L-BFGS optimization</dt-cite> was used in the original paper, we opted for the <dt-cite key="KingmaB14">Adamax optimizer</dt-cite> since it has been shown to converge faster in our experiments.
                </p>

                <h4>Feature blending</h4>
                <p class="toRender">
                  The goal of the feature blending is to reconstruct the latent feature map $F_{A2}^{L-1}$ at layer $L-1$ by fusing the content features of $F_{A1}^{L-1}$ with the style and/or details features of the deconvolved $R_{B1}^{L-1}$ such that:
                  $$F_{A2}^{L-1} = W_{A1}^{L-1} \odot F_{A1}^{L-1} + (1 - W_{A1}^{L-1}) \odot R_{B1}^{L-1}$$
                  where $\odot$ is the element-wise multiplication, and $W_{A1}^{L-1}$ is a weight map.
                </p>
                <p class="toRender">
                  The weight map is computed as a function of the magnitudes of neuron responses at layer $L-1$ to the content structure in image <i>A1</i>. We have :

                  $$ W_{A}^{L-1} = \alpha_{L-1} M_{A}^{L-1} $$
                  $$ M_{A}^{L-1} = \mathrm{sigmoid}\left( \kappa \times \left( |F_{A1}^{L-1}|^{2} - \tau \right) \right) $$

                  where $\kappa=300$ and $\tau=0.05$ and $\lbrace \alpha_{L} \rbrace_{L \in 1..4} = \lbrace 0.1, 0.6, 0.7, 0.8 \rbrace$, values used in the original paper.
                </p>

            <h3 class="sub-section-title">3. Implementation details</h3>
                <p> Our implementation of Deep Image Analogy is in Python, and uses the deep learning framework pytorch for manipulating tensors and easily use and compare optimization algorithms. The PatchMatch steps, being highly dependant on intricate loops are processed on CPU. The reconstruction phase formulated as an optimization problem is computed on GPU. We use the squared Euclidean distance for similarity metric between deep patches. One experiment takes around 10 minutes to run using an Intel core i7 and an Nvidia GTX 1080.</p>


            <hr> <!--Separation line-->
            <!-- SECTION ____________________________________________________________________________-->
            <h2 class="section-title">Results and discussion</h2>
                <p> 
                    The current state of our implementation demonstrates the capability of the technique to do style transfer in a semantically consistent manner. Some result examples can be found on the next figure. We reiterate that <i>A1</i> and <i>B1</i> are the reference images. <i>A2</i> is the image resulting of applying the style of <i>B1</i> to the content of <i>A1</i>. Similarly, <i>B2</i> is obtained by applying the style of <i>A1</i> to the content in <i>B1</i>. All of those results were obtained with the same hyper-parameters.
                </p>

                <figure id="" style="padding:5px 5px 0px 0px">
                  <center style="position:relative; top:-35px">
                    <img class="image" src="images/more_examples.png" hight="400px">
                  </center>
                  <figcaption class="toRender">
                      Some results obtained with our implementation of Deep Image Analogy.
                  </figcaption>
                </figure>

                <p> 
                    As our eyes are very good at detecting what seem un-natural in an image, the first thing that one might focus on are the remaining color artefacts on the resulting images <i>A2</i> and <i>B2</i>. With the time that we had left for this project, we haven't been able to remove every remaining artefacts. However, it is worth noticing that those artefacts are a lot less critical than in the version of the algorithm presented in class (see slides.pdf).
                </p>

                <p> 
                    However, when we focus on the semantic analogies between the reference images, we can see that our implementation is quite successful at adapting to be a mix of the references. For instance, we can see that the eyes and hair of image <i>A2</i> in the second row look similar to the eyes and hair of reference <i>B1</i>, but that the general shape is still the one from <i>A1</i>. Those analogies still work for different objects like boats. In the third row, we can see that the boat sails style are correctly swaped (minus the color artefacts).
                </p>


            <h3 class="sub-section-title">Comparisons</h3>

                <p>
                    To evaluate the successes and flaws of our implementation, we run the algorithm on some examples from the original Deep Image Analogy paper<dt-cite key="liao2017"></dt-cite>. On the two next figures, our implementation results are shown on the first row, and the original results takent from the paper are shown on the second row. The first comparison mixes a .
                </p>

                <figure id="" style="padding:5px 5px 0px 0px">
                  <center style="position:relative; top:-35px">
                    <img class="image" src="images/comparison_original_monalisa.png" hight="400px">
                  </center>
                  <figcaption class="toRender">
                      Style transfer from photo to painting. The first image is a photorealistic computer generated image from the movie Avatar, while the second is the famously known Monalisa painting. First row is our implementation, second row are the results from the original Deep Image Analogy.
                  </figcaption>
                </figure>

                <figure id="" style="padding:5px 5px 0px 0px">
                  <center style="position:relative; top:-35px">
                    <img class="image" src="images/comparison_original_panda.png" hight="400px">
                  </center>
                  <figcaption class="toRender">
                      Style transfer from photo to drawing, both representing pandas. First row is our implementation, second row are the results from the original Deep Image Analogy.
                  </figcaption>
                </figure>

                <p>
                    In both these comparisons, we show that on a high (semantic) level, our results are very similar to those generated by the original authors. However, the small scale details of our generations are considerably more pixelized, and important color artefacts can be found in some images. Those artefacts could be due to the fact that we didn't use the exact same distance metric for PatchMatch than in the original paper. For computal efficiency we chose to use the simpler Euclidean distance rather than using the Gramian-like distance used in the paper, that would be considerably longer to compute. In future works, we might try to implement an efficient yet more complex distance metric to try to improve those small scale details.
                </p>

                <p>
                    We also compare with the original Neural Style Transfer approach<dt-cite key="gatys2016"></dt-cite> and show that our implementation of Deep Image Analogy performs style transfer in a much more coherent way. In the first comparison, we can see that Neural Style Transfer performs better style transfer from painting to photo (see result <i>A2</i>) which is expected, as this algorithm has been designed specifically with those cases in mind. However, when performing style transfer by taking the style from something that is not a uniformly colored painting, we can see that Neural Style Transfer performs much worse than our implementation of Deep Image Analogy. Indeed, the original Neural Style Transfer doesn't take into account what kinf od objects are present in the scene, and therefore, simply blends the colors and traits of the style-reference and applies it uniformly to the content-reference, with total disregard about where each color or each texture should be applied in the resulting image.
                </p>

                <figure id="" style="padding:5px 5px 0px 0px">
                  <center style="position:relative; top:-35px">
                    <img class="image" src="images/gatys_comparison_city.png" hight="400px">
                  </center>
                  <figcaption class="toRender">
                      Style transfer from painting to photo. We can see that on <i>A2</i>, Neural Style Transfer performs a bit better than our implementation of Deep Image Analogy.
                  </figcaption>
                </figure>

                <figure id="" style="padding:5px 5px 0px 0px">
                  <center style="position:relative; top:-35px">
                    <img class="image" src="images/gatys_comparison_avatar.png" hight="400px">
                  </center>
                  <figcaption class="toRender">
                      Style transfer from painting to photo with semantic correspondances. We can see that on Neural Style Transfer isn't doing the right thing at all. It doesn't know where are the eyes, ears, hair and mouth and instead almost uniformly applies some sort of blending of the style-reference to the content-reference.
                  </figcaption>
                </figure>

                <figure id="" style="padding:5px 5px 0px 0px">
                  <center style="position:relative; top:-35px">
                    <img class="image" src="images/gatys_comparison_manga.png" hight="400px">
                  </center>
                  <figcaption class="toRender">
                      Style transfer from drawing to drawing with semantic correspondances. Again our implementation performs much better.
                  </figcaption>
                </figure>


            <h3 class="sub-section-title">Limitations</h3>

            <p>
                We show some results on face to face style transfers with portraits of our colleagues.
            </p>
            

                <figure id="" style="padding:5px 5px 0px 0px">
                  <center style="position:relative; top:-35px">
                    <img class="image" src="images/lab_team.png" hight="400px">
                  </center>
                  <figcaption class="toRender">
                      Face to face style transfer. 1rst row, from left to right : Raymond Li, Raymond Roy, Julien Li and Julien Roy. 2nd row, from left to right : Félix Harvey, Félix Kanaa, David Harvey and David Kanaa.
                  </figcaption>
                </figure>

            <p>
                Again, we notice limitations of small-scale textures and large color-artefacts. Our implementation seem to sometimes mix regions of homogenous color (for instance, on the second row, the homogenous shirt contain a color-patch of the homogenous background wall). This effect might be due to the fact that in the deeper layers of VGG, the feature maps might not contain very specific activations for representing those homogenous regions such as uniformly colored clothes or backgrounds.
            </p>


            <p>
                Moreover, a known limitation of Deep Image Analogy happens when one of the reference image contain an object of semantical importance that is not contained in the other reference. For instance, if one of the subject had a hat or other large accessory, the algorithm would be incapable of finding a corresponding region in the other reference and would probably fill the hat region with some other similar texture that would lead to weird shapes in the resulting images.
            </p>

            <p>
                Finally, because VGG is not trained to be invariant to elastic transformations in the input, the algorithm struggles when the two references contain semantic correspondances but with very different shapes. An example of such case is presented in the next figure.
            </p>

                <figure id="" style="padding:5px 5px 0px 0px">
                  <center style="position:relative; top:-35px">
                    <img class="image" src="images/homer_peter.png" hight="400px">
                  </center>
                  <figcaption class="toRender">
                      Cartoon to cartoon style transfer. Because both faces have very different shapes, the algorithm tries to widen <i>A2</i> in order to make it look more similar to <i>B1</i>. Also, the absence of beard region in <i>B1</i> makes it hard to know where to represent <i>A1's</i> beard in <i>B2</i>.
                  </figcaption>
                </figure>

            <hr> <!--Separation line-->
            <!-- SECTION ____________________________________________________________________________-->
            <h2 class="sub-section-title">Conclusion</h2>
                <p> We implemented a working version of the Deep Image Analogy algorithm. While our current implementation still produces important artefacts, we demonstrated that it successfully accomplishes the main transformation that is was designed to do, which is to perform style transfer between images in a semantically consistent manner. Our future works aim on improving the quality of our results by trying new distance metrics for the PatchMatch phase and look into post-processing possibilities to improve the small scale details.</p>



        <dt-appendix class="left">
        </dt-appendix>
        <h3>Acknowledgments</h3>
            <p>We want to thank Pr. Christopher Pal for allowing us to use GPUs from his server to perform our experiments. We also thank our colleagues from the lab which kindly accepted to allow us to experiment on their portrait in the name of Science.</p>

        <script type="text/javascript">
            var el = document.getElementsByClassName("toRender")
            for (var i = 0; i < el.length; i++) {
                renderMath(el[i]);
            }
        </script>

        <script type="text/bibliography">

            @article{liao2017,
              title={Visual Attribute Transfer through Deep Image Analogy},
              author={Liao, Jing and Yao, Yuan and Yuan, Lu and Hua, Gang and Kang, Sing Bing},
              journal={arXiv preprint arXiv:1705.01088},
              year={2017}
            }

            @inproceedings{gatys2016,
              title={Image style transfer using convolutional neural networks},
              author={Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
              booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
              pages={2414--2423},
              year={2016}
            }

            @article{barnes2009,
              title={PatchMatch: A randomized correspondence algorithm for structural image editing},
              author={Barnes, Connelly and Shechtman, Eli and Finkelstein, Adam and Goldman, Dan B},
              journal={ACM Trans. Graph.},
              volume={28},
              number={3},
              pages={24--1},
              year={2009}
            }

            @article{SimonyanZ14a,
              author    = {Karen Simonyan and
                           Andrew Zisserman},
              title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
              journal   = {CoRR},
              volume    = {abs/1409.1556},
              year      = {2014},
              url       = {http://arxiv.org/abs/1409.1556},
              archivePrefix = {arXiv},
              eprint    = {1409.1556},
              timestamp = {Wed, 07 Jun 2017 14:41:51 +0200},
              biburl    = {http://dblp.org/rec/bib/journals/corr/SimonyanZ14a},
              bibsource = {dblp computer science bibliography, http://dblp.org}
            }

            @inproceedings{pouli2010,
              title={Progressive histogram reshaping for creative color transfer and tone reproduction},
              author={Pouli, Tania and Reinhard, Erik},
              booktitle={Proceedings of the 8th International Symposium on Non-Photorealistic Animation and Rendering},
              pages={81--90},
              year={2010},
              organization={ACM}
            }

            @article{Zhu1997,
             author = {Zhu, Ciyou and Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge},
             title = {Algorithm 778: L-BFGS-B: Fortran Subroutines for Large-scale Bound-constrained Optimization},
             journal = {ACM Trans. Math. Softw.},
             issue_date = {Dec. 1997},
             volume = {23},
             number = {4},
             month = dec,
             year = {1997},
             issn = {0098-3500},
             pages = {550--560},
             numpages = {11},
             url = {http://doi.acm.org/10.1145/279232.279236},
             doi = {10.1145/279232.279236},
             acmid = {279236},
             publisher = {ACM},
             address = {New York, NY, USA},
             keywords = {large-scale optimization, limited-memory method, nonlinear optimization, variable metric method},
            }

            @article{KingmaB14,
              author    = {Diederik P. Kingma and
                           Jimmy Ba},
              title     = {Adam: {A} Method for Stochastic Optimization},
              journal   = {CoRR},
              volume    = {abs/1412.6980},
              year      = {2014},
              url       = {http://arxiv.org/abs/1412.6980},
              archivePrefix = {arXiv},
              eprint    = {1412.6980},
              timestamp = {Wed, 07 Jun 2017 14:40:52 +0200},
              biburl    = {http://dblp.org/rec/bib/journals/corr/KingmaB14},
              bibsource = {dblp computer science bibliography, http://dblp.org}
            }


        </script>


        </dt-article>
    </body>
</html>
