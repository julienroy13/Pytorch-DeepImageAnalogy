<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=768, initial-scale=1">
        <script src="http://distill.pub/template.v1.js"></script>
        <script type="text/front-matter">
          title: Deep Image Analogy
          description: A Semantically Consistent Approach to Neural Style Transfer
          authors:
            - Julien Roy: https://github.com/julienroy13
            - David Kanaa: https://github.com/davidkanaa
          affiliations:
              - Polytechnique Montreal: "http://ww.polymtl.ca"
              - Polytechnique Montreal: "http://ww.polymtl.ca"
        </script>
        <!-- Katex -->
        <script src="assets/lib/auto-render.min.js"></script>
        <script src="assets/lib/katex.min.js"></script>
        <link rel="stylesheet" href="assets/lib/katex.min.css">
        <link rel="stylesheet" type="text/css" href="assets/widgets.css">
        <link rel="stylesheet" type="text/css" href="assets/main.css">

        <!-- custom -->
        <link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">

        <!-- Required -->
        <script src="assets/lib/lib.js"></script>
        <script src="assets/utils.js"></script>
        <script>
        var renderQueue = [];

        function renderMath(elem) {
            renderMathInElement(
                elem, {
                    delimiters: [{
                        left: "$$",
                        right: "$$",
                        display: true
                    }, {
                        left: "$",
                        right: "$",
                        display: false
                    }, ]
                }
            );
        }

        var deleteQueue = [];

        function renderLoading(figure) {
            var loadingScreen = figure.append("svg")
                .style("width", figure.style("width"))
                .style("height", figure.style("height"))
                .style("position", "absolute")
                .style("top", "0px")
                .style("left", "0px")
                .style("background", "white")
                .style("border", "0px dashed #DDD")
                .style("opacity", 1)

            return function(callback) {
                loadingScreen
                    .remove()
            }

        }
        </script>

        <style>

        #previews figcaption {
          text-align: center;
        }

        .tex, .tex > * {
          font-family: "Computer Modern Serif", serif;
        }

        p, p > * {
          text-align: justify;
        }

        .section-title, .sub-section-title {
          font-weight: bold;
        }

        </style>

    </head>
    <body>
        <dt-article class="left tex">
            <h1>Deep Image Analogy</h1>
            <h2>A Semantically Consistent Approach to Neural Style Transfer</h2>
            <dt-byline></dt-byline>

                <p>
                    Image style transfer refers to the problem of transforming an image in such a way that it acquires the stylistic attributes of a reference image, while preserving its content information. For instance, color editing with respect to a reference image can be achieved by matching the target and the reference color histograms <dt-cite key="pouli2010"></dt-cite>. However, even if defining a complete definition of <i>style</i> would be difficult, most would agree that the stylistic attributes account for more than simply color. Some of these attributes however might be hard to describe or identify -- the style of a painting could lie in the brush strokes and the textures, the style of a photograph is partly defined by the lighting and tone nuances.
                </p>
                <p>
                    The fact that humans can appreciate and differentiate style without quite agreeing on how to describe it (even less on how to formalize it) makes machine learning a promising approach for that problem.
                </p>
                <p>
                    Gatys <i>et al.</i> developped the first algorithm that uses a deep convolutional neural network (CNN) to capture and modelize the style of an image. This technique is refered to as <dt-cite key="gatys2016">Neural Style Transfer</dt-cite>. However, while being able to produce impressive results, this approach fails to carry out style transfer in a manner that preserves the semantic correspondances between the two original images. For example, the algorithm won't distinguish the difference between the background and the main objects of both scenes, and thus will apply the style in an homogenous manner to the generated image without any regard about where which part of the style should be applied.
                </p>
                <p>
                    Liao <i>et al.</i> solve that issue by introducing <dt-cite key="liao2017">Deep Image Analogy</dt-cite>. This algorithm is inspired by the original Neural Style Transfer as it still uses a pretrained deep CNN to extract information about the images, but also cleverly apply the Patch Match method<dt-cite key="barnes2010"></dt-cite> on deep feature maps constructed by that CNN to take into account semantic correspondances between the two base images. Therefore, they iteratively progress in a coarse-to-fine manner (from the deeper to shallower representations) to generate the new latent image that both presents the style of one reference, and the content of another, in a semantically consistent fashion.
                </p>
                <hr> <!--Separation line-->

            <!-- SECTION ____________________________________________________________________________-->
            <h2 class="section-title">Background</h2>
                <p> In this section, we briefly present the pre-requisite notions that are at the core of the Deep Image Analogy method.  </p>

            <h3 class="sub-section-title">1. Randomized Patch-Match algorithm</h3>
                <p class="toRender">
                    Patch matching refers to the problem of finding the nearest neighbor patch in an image <i>B</i> to a source patch in an image <i>A</i>. The result of this process is a mapping function $\phi_{a \to b}$ called Nearest-Neighbor Field (NNF) that is of the same size as image <i>A</i>, and which defines a mapping from pixels in <i>A</i> to those in <i>B</i> : let <i>p</i> be a pixel in <i>A</i> and $N(p)$ a fixed patch centered on <i>p</i>, $\phi_{a \to b}(p)=q$ is the pixel in <i>B</i> such that $N(q)$ is the nearest patch in <i>B</i> to $N(p)$, with respect to some similarity $d(.,.)$. The patches are usually extended to all channels of an image, so for a 3 channels RGB image, a $3\times3$ patch would in fact refer to a $3\times3\times3$ tensor. Different similarity metrics can be used to define the nearest patch may be used, a simple one being the squared Euclidean distance :

                    $$ d(P^{A}, P^{B}) = \| P^{A}-P^{B} \|_{2}^{2}= \sum_{i,j,k} (P^{A}_{ijk} - P^{B}_{ijk})^{2}$$
                </p>
                <p>
                    The main challenge is the number of patches to compare with each other, which grows exponentially with the size of the images. Luckily, the Randomized approach to <dt-cite key="barnes2010">PatchMatch</dt-cite> allows to find very good estimate solutions in a much shorter amount of time. The strategy is simple, the NNF is initialized randomly. Then, for each pixels, we execute two steps : propagation and random search. Propagation refers to looking at the pixel's neighbors to see if they could find a better match than it's own. Random search consists in randomly selecting potential match candidates in a certain area, and see if they offer a better match than the current one. Those steps are illustrated in the next figure.

                    <figure>
                        <center><img class="image" src="images/patch_match.png" height="400px"></center>
                        <figcaption style="text-align: center;">Randomized Patch-Match illustrated as a 3-step process (Figure from :<dt-cite key="barnes2010"></dt-cite>)</figcaption>
                    </figure>
                </p>
                <p>
                    This approach depends on the reasonable assumption that natural images are usually spatially smooth. That means that small sub-regions of the image will usually have similar colors, so successive patches in image <i>A</i> might want to share the same correspondances in image <i>B</i> (propagation step) and that randomly sampling patches to compare with that come from different sub-regions counts as an efficient scan of the image in order to find a good neighborhood (random search step). When the PatchMatch is successful, one can express image <i>A</i> only in terms of pixels in image <i>B</i>, a process shown in the next figure. This resulting hybrid image is called a <i>warp</i>.
                </p>
                <figure>
                    <center><img class="image" src="images/warp.png"></center>
                    <figcaption class="toRender" style="text-align: center;">
                        The result of applying the <dt-cite key="barnes2010">Ranomized PacthMatch</dt-cite> algorithm on two images : it maps <i>image A</i> to <i>image B</i>.
                        A warping operation using the constructed NNF is applied on <i>image B</i> to recover <i>image A</i>.
                    </figcaption>
                </figure>

            <h3 class="sub-section-title">2. Semantic feature extraction with deep CNNs</h3>
                <!-- here give some background info on deep CNNs used in image recognition tasks and VGG trained for classification and recently used for style Transfer -->
                <p class="toRender">
                  Despite the fact that the <dt-cite key="barnes2010">Ranomized PacthMatch</dt-cite> algorithm performs well at matching pixels across two images such that one can be recovered (as best as possible) by warping the other (using the correcpondant nearest neighbour field), it only operates on raw pixels and does not take into account the inherent semantic structure of the provided images.
                </p>
                <p class="toRender">
                  <dt-cite key="SimonyanZ14a">VGG-19</dt-cite> is a deep convolutional neural network architecture which have been trained and evaluated on the ImageNet data set for image classification/recognition tasks, which it has been shown to perform very well at. VGG has also been used for the <dt-cite key="gatys2016">style transfer task</dt-cite> with success.
                  It consists of 5 convolutional blocks (made of 2, 2, 4, 4 and 4 layers respectively) supplemented with a $2\times2$ pooling layer (max or average) each, and followed by a couple of fully connected layers.
                  The convolutional blocks of VGG use spatial coherence in images to build higher representations that do expose latent semantic information (such as eyes and nose positions for faces, colours ...etc).
                  Each block builds a higher representation on top of the previous, extracting more and more semantics, which are later used in fully connected layers for classification.
                </p>
                <figure id="" style="padding:5px 5px 0px 0px">
                  <center style="position:relative; top:-35px">
                    <img class="image" src="images/vgg.png" hight="400px">
                  </center>
                  <figcaption class="toRender" style="text-align: center;">
                      An illustration of the VGG deep neural architecture.
                  </figcaption>

                </figure>
                <p class="toRender">
                  We use a pretrained instance of the VGG-19 architecture to build and extract feature maps. Since the network is not used for classification purposes, we can get rid of the fully connected layers, and only work with the convolutional blocks of VGG.
                  We do not retrieve the feature maps produced at the output of every layer of the network, but only those produced at the first layer of convolutional blocks (the output of the $reluL\_1$).
                </p>
                <figure id="" style="padding:5px 5px 0px 0px">
                  <center style="position:relative; top:-35px">
                    <img class="image" src="images/deep_features.png" hight="400px">
                  </center>
                  <figcaption class="toRender" style="text-align: center;">
                      An illustration of feature maps $F^L, \forall L \in \lbrace 1, 2, ..., 5\rbrace$ extracted with the <dt-cite key="SimonyanZ14a">VGG-19</dt-cite> architecture.
                  </figcaption>
                </figure>

            <hr> <!--Separation line-->
            <!-- SECTION ____________________________________________________________________________-->
            <h2 class="section-title">Algorithm</h2>
                <p class="toRender">
                    The algorithm has been designed to perform style transfer on two images, while respecting the semantic correspondances between them. Given two reference images <i>A1</i> and <i>B1</i>, it produces two mapping functions $\phi_{a \to b}$ and $\phi_{b \to a}$ that allow to warp the references into two new images <i>A2</i> and <i>B2</i> that mix the style and the content of the references. An overview of the algorithm is presented on the following figure.
                </p>
                <figure>
                    <center><img class="image" src="images/overview.png"></center>
                    <figcaption style="text-align: center;">Overview of Deep Image Analogy (Figure from :<dt-cite key="liao2017"></dt-cite>)</figcaption>
                </figure>
                <p class="toRender">
                    Broadly, the process aims at reconstructing all deep feature maps of both provided images, starting at the fifth layer and all the way back to the first layer of VGG. Therefore, before starting this process, the two reference images are fed to the network and their deep features are saved. The feature maps of the two latent images at the deepest layer ($L=5$) are initialized to the same values as the reference images (more specifically, $F^{(5)}_{A2}$ is initialized with $F^{(5)}_{A1}$ and $F^{(5)}_{B2}$ is initialized with $F^{(5)}_{B1}$). Then, for each layer $L$, an NNF-search (Deep PatchMatch) is performed to discover the mapping functions $\phi^{(L)}_{a \to b}$ and $\phi^{(L)}_{b \to a}$, followed by an optimization phase (Deep Reconstruction) that aims at reconstructing the feature maps $F^{(L-1)}_{A2}$ and $F^{(L-1)}_{B2}$ of the images at layer $L-1$. The Deep PatchMatch and Deep Reconstruction phases are explained in more details in the next sections.
                </p>

            <h3 class="sub-section-title">1. Deep PatchMatch</h3>
                <p>
                    One key element of the method is that the PatchMatch is applied to find correspondances between the deep features of the reference and latent images. Applying this technique at such a deep level of image features allows to map semantic elements of both images to one another. The next figure shows how the eyes of the two reference images are very similarly represented at that depth.
                </p>
                <figure>
                    <center><img class="image" src="images/semantic_correspondances.png"></center>
                    <figcaption style="text-align: center;">Both faces, very dissimilar at the pixel level, are now represented in very similar ways at the 5th layer. (Images from <dt-cite key="liao2017"></dt-cite>)</figcaption>
                </figure>
                <p class="toRender">
                    Another key element is that the random search in deep patch match is locally constrained. The mapping function for the fifth layer is randomly initialized and has access to the whole image in the random search phase in order to find the best possible matches for each semantic attributes. However, the mapping functions for $L \in \{1,2,3,4\}$ are initilized with an upsampled version of the previous layer, and their random search only has access to a small centered at the current best match, making it impossible to completely destroy the mapping that had been established in previous layers. Therefore, the correspondances found at the deepest layers serve as the foundation upon which the next mapping functions are built, and those shallower layers mapping only are fine-tuned versions of the deeper ones. This fine-tuning effect is illustrated in the following figure.
                </p>
              <figure>
                    <center><img class="image" src="images/spatial_constraint.png"></center>
                    <figcaption style="text-align: center;">The fifth layer finds the semantic correspondances. This mapping is then fine-tuned to fit smaller details and textures as the process takes us to shallower and shallower layers.</figcaption>
              </figure>

            <h3 class="sub-section-title">2. Deep Reconstruction</h3>
                <!-- here explain every step an notion involved in the feature reconstruction -->
                <p class="toRender">
                  For every layer $L \in \lbrack 2..5 \rbrack$, assume we have the pair of feature maps $F_{A1}^{L-1}$, $F_{B1}^{L}$, respectively of image <i>A1</i> at later $L-1$ and <i>B1</i> at layer $L$.
                  The goal of the feature reconstruction is to recover features of <i>A2</i> at layer $L-1$ before further computation of NNFs at this layer. This phase consists in 3 steps : <i>(1)</i> the feature map warping using NNFs computed in previous layers, <i>(2)</i> the deconvolution and <i>(3)</i> the feature blending.
                  Here we explain each of them.
                </p>
                <figure id="" style="padding:5px 5px 0px 0px">
                  <center style="position:relative; top:-35px">
                    <img class="image" src="images/reconstruction.png" hight="400px">
                  </center>
                  <figcaption class="toRender" style="text-align: center;">
                      An overview of the feature reconstruction process.
                  </figcaption>
                </figure>
                <h4>Feature warping</h4>
                <p class="toRender">
                  Provided we have $\phi^{L}_{a \to b}$ the nearest neighbour field at layer $L$, we apply a warping operation to $F_{B1}^{L}$ such that:
                  $$\forall \text{ pixel } p \in F_{B1}^{L}, \text{ warp}(F_{B1}^{L}(p)) = F_{B1}^{L} \circ \phi_{a \to b}(p) = F_{B1}^{L}(\phi_{a \to b}(p))$$
                  Thus, the warped feature map $F_{B1}^{L}(\phi_{a \to b})$. The warped feature maps represents a version of $F_{B1}^{L}$ where every semantic concept has been shifted to match the location of its correspondant in $F_{A1}^{L}$.
                  It makes sense that if we are to blend feature maps of both images we must make sure that their matching semantic concepts have the same location, in order for the resultant feature map to keep some consistency in terms of semantic information.
                </p>
                <figure id="" style="padding:5px 5px 0px 0px">
                  <center style="position:relative; top:-35px">
                    <img class="image" src="images/warping.png" hight="400px">
                  </center>
                  <figcaption class="toRender" style="text-align: center;">
                      An illustration of the effect of warping (here using the NNFs of layer 5).
                  </figcaption>
                </figure>

                <h4>Deconvolution</h4>
                <p class="toRender">
                  Since the goal is to recover $F_{A2}^{L-1}$, that is the feature maps at the previous layer, we must deconvolve the features of the current layer <i>L</i> using the appropriate subnet $CNN_{L-1}^{L}$ of VGG-19. This deconvolution task is here formulated as an optimization problem, in which the targets are the warped features $F_{B1}^{L}(\phi_{a \to b})$, and the loss function :
                  $$ \mathcal{L}_{{R_{B2}^{L-1}}} = \| CNN_{L-1}^{L}(R_{B1}^{L}) - F_{B1}^{L}(\phi_{a \to b}^{L}) \|^{2} $$
                  Using the convolutional network here allows, again, to take advantage of the local consistency of the semantic information in features. As the network already knows how to build the feature maps and extract semantic information, it can be used in this fashion, to reverse its doing. Via the deconvolution viewed as an optimization problem, we can determine the features $R_{B1}^{L}$ which should be fed into the previous layer in order to obtain the warped features we have at the current layer. We initialize $R_{B1}^{L}$ with a random [gaussian] noise, and solve the numerical optimization problem through gradient descent $\partial \mathcal{L}_{{R_{B2}^{L-1}}}/{ \partial R_{B2}^{L-1} }$.
                </p>
                <p class="toRender">
                  Though the <dt-cite key="Zhu1997">L-BFGS optimization</dt-cite> was used in the original paper, we opted for the <dt-cite key="KingmaB14">Adamax optimizer</dt-cite> since it has been shown to converge faster in our experiments.
                </p>

                <h4>Feature blending</h4>
                <p class="toRender">  </p>

            <h3 class="sub-section-title">3. Implementation details</h3>
                <p> Our implementation of Deep Image Analogy is in Python, and uses the deep learning framework pytorch for manipulating tensors and easily use and compare optimization algorithms. The PatchMatch steps, being highly dependant on intricate loops are processed on CPU. The reconstruction phase formulated as an optimization problem is computed on GPU. We use the squared Euclidean distance for similarity metric between deep patches.</p>


            <hr> <!--Separation line-->
            <!-- SECTION ____________________________________________________________________________-->
            <h2 class="section-title">Results and discussion</h2>
                <p> Blablabla</p>

            <h3 class="sub-section-title">Comparisons</h3>

            <h3 class="sub-section-title">Limitations</h3>


            <hr> <!--Separation line-->
            <!-- SECTION ____________________________________________________________________________-->
            <h2 class="sub-section-title">Conclusion</h2>
                <p> Blablabla</p>


            <!-- SECTION ____________________________________________________________________________-->
            <h3>Examples of formatting</h3>
                <p class="toRender">
                    In a regular RNN, at <i>italic text</i>, the cell state $h_t$ is computed based on its own input and the cell state $h_t$ that encapsulates some information from the precedent inputs : $$h_t = f(W^{hx}x_t + W^{hh}h_{t-1})$$
                </p>

                <p>
                    The following figure presents a Seq2Seq model with a two layers encoder and a two layers decoder:
                    <img class="image" src="images/exemple.jpg">
                </p>
                <p>
                    However, to better overcome the information bottleneck and long-term dependencies limitations, attentions model have been introduced. The basic idea of attention is that instead of attempting to learn a single vector representation for each sentence, we keep around vectors for every word in the input sentence, and reference these vectors at each decoding step.
                </p>
                <ul>
                    <li>
                        <p class="toRender">
                            Blablabla
                        </p>
                    </li>
                    <li>
                        <p class="toRender">
                            Blablabla $b_{t} = (b_{t0}, ... ,b_{tT})^T$ is then passed throught a softmax function.
                            $$\alpha_{tj} = softmax(b_{tj}) = \frac{exp(b_{tj})}{\sum_{k=1}^{T} exp(b_{tk})}$$
                        </p>
                    </li>
                    <li>
                        <p class="toRender">
                            Blablabla
                        </p>
                    </li>
                </ul>
                <p>
                    Remember that for :
                    </p><figure id="iteratespluserror" style="width:610px; height:105px; display:block; margin-left:auto; margin-right:auto; position:relative">
                        <div style="position:relative; top:-35px">
                            <span class="toRender" style="position:absolute; left:0px; top:0px">
                                $$p(y_t = y_k|y_{&lt; t}, x)$$
                            </span>
                            <figcaption class="toRender" style="position:absolute; left:00px; top:105px; width:120px;">
                                Caption here
                            </figcaption>
                            <span class="toRender" style="position:absolute; left:170px; top:0px">
                                $$=$$
                            </span>
                            <span class="toRender" style="position:absolute; left:200px; top:0px">
                                $$e^{w_k^Tg(y_{t-1}, h_t, c_t)}$$
                            </span>
                            <figcaption class="toRender" style="position:absolute; left:200px; top:105px;width:210px;">
                                Caption there
                            </figcaption>
                            <span class="toRender" style="position:absolute; left:400px; top:0px">
                                $$/$$
                            </span>
                            <span class="toRender" style="position:absolute; left:440px; top:-10px">
                                $$\displaystyle{\sum_{k^{'} \in K} e^{w_{k^{'}}^Tf(y_{t-1}, h_t, c_t)}}$$
                            </span>
                            <figcaption class="toRender" style="position:absolute; left:440px; top:105px;width:200px;">
                                Finally, the sum of all $|K|$ logits.
                            </figcaption>
                        </div>
                    </figure>

                <p>
                    The main idea of the proposed approach is to approximate the second term of the gradient (i.e
                    <dt-fn>
                    Small explanation in a box
                    </dt-fn>)
                    , by importance sampling
                    <dt-fn>
                    Second eexplanation in a tiny beautiful box
                </p>
                <p class="toRender">
                    <b>
                        Bold stuff:
                    </b>
                    Blablablabla blablabla blabla blablabla blabla blablabla blabla blablabla blabla blablabla blabla blablabla blabla blablabla blabla blablabla blabla blablabla blabla blablabla blabla blablabla blabla blablabla blabla blablabla blabla blablabla blabla blablabla blabla blablabla blabla blablabla blabla.
                </p>


        <dt-appendix class="centered">
            </dt-appendix>
            <h3>Acknowledgments</h3>
                <p>Thank you to Pr. Christopher Pal for allowing us to use GPUs from his server to perform our experiments.</p>

        <script type="text/javascript">
            var el = document.getElementsByClassName("toRender")
            for (var i = 0; i < el.length; i++) {
                renderMath(el[i]);
            }
        </script>

        <script type="text/bibliography">

            @article{liao2017,
              title={Visual Attribute Transfer through Deep Image Analogy},
              author={Liao, Jing and Yao, Yuan and Yuan, Lu and Hua, Gang and Kang, Sing Bing},
              journal={arXiv preprint arXiv:1705.01088},
              year={2017}
            }

            @inproceedings{gatys2016,
              title={Image style transfer using convolutional neural networks},
              author={Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
              booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
              pages={2414--2423},
              year={2016}
            }

            @inproceedings{barnes2010,
              title={The generalized patchmatch correspondence algorithm},
              author={Barnes, Connelly and Shechtman, Eli and Goldman, Dan B and Finkelstein, Adam},
              booktitle={European Conference on Computer Vision},
              pages={29--43},
              year={2010},
              organization={Springer}
            }

            @article{SimonyanZ14a,
              author    = {Karen Simonyan and
                           Andrew Zisserman},
              title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
              journal   = {CoRR},
              volume    = {abs/1409.1556},
              year      = {2014},
              url       = {http://arxiv.org/abs/1409.1556},
              archivePrefix = {arXiv},
              eprint    = {1409.1556},
              timestamp = {Wed, 07 Jun 2017 14:41:51 +0200},
              biburl    = {http://dblp.org/rec/bib/journals/corr/SimonyanZ14a},
              bibsource = {dblp computer science bibliography, http://dblp.org}
            }

            @inproceedings{pouli2010,
              title={Progressive histogram reshaping for creative color transfer and tone reproduction},
              author={Pouli, Tania and Reinhard, Erik},
              booktitle={Proceedings of the 8th International Symposium on Non-Photorealistic Animation and Rendering},
              pages={81--90},
              year={2010},
              organization={ACM}
            }

            @article{Zhu1997,
             author = {Zhu, Ciyou and Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge},
             title = {Algorithm 778: L-BFGS-B: Fortran Subroutines for Large-scale Bound-constrained Optimization},
             journal = {ACM Trans. Math. Softw.},
             issue_date = {Dec. 1997},
             volume = {23},
             number = {4},
             month = dec,
             year = {1997},
             issn = {0098-3500},
             pages = {550--560},
             numpages = {11},
             url = {http://doi.acm.org/10.1145/279232.279236},
             doi = {10.1145/279232.279236},
             acmid = {279236},
             publisher = {ACM},
             address = {New York, NY, USA},
             keywords = {large-scale optimization, limited-memory method, nonlinear optimization, variable metric method},
            }

            @article{KingmaB14,
              author    = {Diederik P. Kingma and
                           Jimmy Ba},
              title     = {Adam: {A} Method for Stochastic Optimization},
              journal   = {CoRR},
              volume    = {abs/1412.6980},
              year      = {2014},
              url       = {http://arxiv.org/abs/1412.6980},
              archivePrefix = {arXiv},
              eprint    = {1412.6980},
              timestamp = {Wed, 07 Jun 2017 14:40:52 +0200},
              biburl    = {http://dblp.org/rec/bib/journals/corr/KingmaB14},
              bibsource = {dblp computer science bibliography, http://dblp.org}
            }


        </script>


        </dt-article>
    </body>
</html>
